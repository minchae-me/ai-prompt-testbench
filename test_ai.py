# test_ai.py - AI Prompt Test Bench (Python 3.10+)
import re
import time
import hashlib
import json
import os
from typing import Dict, Any, List
from concurrent.futures import ThreadPoolExecutor

import streamlit as st
from google.cloud import bigquery
from openai import OpenAI
from dotenv import load_dotenv
from pathlib import Path

# .env íŒŒì¼ ë¡œë“œ (í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œ)
env_path = Path(__file__).parent / ".env"
load_dotenv(env_path)

PROJECT_ID = os.environ.get("GCP_PROJECT") or os.environ.get("GOOGLE_CLOUD_PROJECT")
AI_DATASET = "ai_sandbox"
BQ_DATASET = "working_gyg"

# í™˜ê²½ ë³€ìˆ˜ í™•ì¸
if not PROJECT_ID:
    st.error("âŒ í™˜ê²½ ë³€ìˆ˜ GCP_PROJECT ë˜ëŠ” GOOGLE_CLOUD_PROJECTë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
    st.info("ğŸ’¡ .env íŒŒì¼ì— GCP_PROJECT=your_project_id ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
    st.stop()

OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    st.error("âŒ í™˜ê²½ ë³€ìˆ˜ OPENAI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
    st.info("ğŸ’¡ .env íŒŒì¼ì— OPENAI_API_KEY=your_api_key ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
    st.code(
        """
# .env íŒŒì¼ ì˜ˆì‹œ:
OPENAI_API_KEY=sk-your_api_key_here
GCP_PROJECT=your_project_id
    """
    )
    st.stop()

# --- GCP/BQ í´ë¼ì´ì–¸íŠ¸
try:
    bq = bigquery.Client(project=PROJECT_ID)
except Exception as e:
    st.error(f"âŒ BigQuery í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    st.info("ğŸ’¡ Google Cloud ì¸ì¦ì„ í™•ì¸í•˜ì„¸ìš”: gcloud auth application-default login")
    st.stop()

# --- OpenAI í´ë¼ì´ì–¸íŠ¸
try:
    client = OpenAI(api_key=OPENAI_API_KEY)
except Exception as e:
    st.error(f"âŒ OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    st.stop()


# 1) ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (+ìºì‹œ)
@st.cache_data(ttl=600)
def fetch_all_models_from_openai() -> list[str]:
    ids = []
    try:
        # SDKê°€ generatorë¥¼ ë°˜í™˜ -> forë¡œ ìˆœíšŒ
        for m in client.models.list():
            ids.append(m.id)
    except Exception as e:
        st.warning(f"/v1/models ì¡°íšŒ ì‹¤íŒ¨: {e}")
    return sorted(set(ids))


# 2) chat/completionsì— ì“¸ ìˆ˜ ì—†ëŠ” ëª¨ë¸ë§Œ ì œì™¸ (ìœ ì—°í•œ í•„í„°ë§)
def filter_chat_models(model_ids: list[str]) -> list[str]:
    # ëª…ë°±íˆ ì±„íŒ…ìš©ì´ ì•„ë‹Œ ëª¨ë¸ë“¤ë§Œ ì œì™¸
    blocked = re.compile(r"(whisper|tts|embedding|dall-e|image|moderation|audio)", re.I)
    # ì°¨ë‹¨ëœ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ëª¨ë¸ë§Œ ì œì™¸í•˜ê³  ë‚˜ë¨¸ì§€ëŠ” ëª¨ë‘ í—ˆìš©
    keep = [m for m in model_ids if not blocked.search(m)]
    # ë‹¨ìˆœíˆ ì•ŒíŒŒë²³ ìˆœìœ¼ë¡œ ì •ë ¬ (ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ëª¨ë¸ì„ ì‰½ê²Œ ì°¾ì„ ìˆ˜ ìˆë„ë¡)
    return sorted(keep)


# OpenAI ëª¨ë¸ ëª©ë¡ì„ ë¯¸ë¦¬ ê°€ì ¸ì˜¤ê¸° (ì‚¬ì´ë“œë°”ì—ì„œ ì¬ì‚¬ìš©)
@st.cache_data(ttl=600)
def get_available_models():
    return filter_chat_models(fetch_all_models_from_openai())


# ---------- BigQuery helpers ----------
def bq_query(
    sql: str, params: Dict[str, Any] | None = None
) -> List[bigquery.table.Row]:
    job_config = bigquery.QueryJobConfig(
        query_parameters=[
            bigquery.ScalarQueryParameter(k, "STRING", v)
            for k, v in (params or {}).items()
        ]
    )
    return list(bq.query(sql, job_config=job_config).result())


def insert_rows_json(table: str, rows: List[Dict[str, Any]]):
    table_ref = bq.dataset(AI_DATASET).table(table)
    errors = bq.insert_rows_json(table_ref, rows)
    if errors:
        raise RuntimeError(f"BigQuery insert error: {errors}")


def md5(text: str) -> str:
    return hashlib.md5(text.encode("utf-8")).hexdigest()


# ---------- Load model list ----------
def load_models() -> List[str]:
    try:
        rows = bq_query(
            f"""
            SELECT DISTINCT model_name 
            FROM `{PROJECT_ID}.{AI_DATASET}.model_pricing`
            WHERE effective_to IS NULL OR effective_to > CURRENT_TIMESTAMP()
            ORDER BY model_name
        """
        )
        return [r["model_name"] for r in rows]
    except Exception as e:
        st.warning(f"âš ï¸ BigQueryì—ì„œ ëª¨ë¸ ëª©ë¡ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        return []


# ---------- Prompt CRUD ----------
def list_project_names() -> List[str]:
    """í”„ë¡œì íŠ¸ ëª©ë¡ ì¡°íšŒ"""
    try:
        rows = bq_query(
            f"""
            SELECT DISTINCT project_name
            FROM `{PROJECT_ID}.{AI_DATASET}.prompt`
            WHERE project_name IS NOT NULL
            ORDER BY project_name
        """
        )
        return [r["project_name"] for r in rows]
    except Exception as e:
        st.warning(f"âš ï¸ í”„ë¡œì íŠ¸ ëª©ë¡ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        return []


def list_prompt_names(prompt_type: str, project_name: str = None) -> List[str]:
    """í”„ë¡¬í”„íŠ¸ ì´ë¦„ ëª©ë¡ ì¡°íšŒ (í”„ë¡œì íŠ¸ë³„ í•„í„°ë§ ê°€ëŠ¥)"""
    try:
        if project_name:
            rows = bq_query(
                f"""
                SELECT DISTINCT prompt_name
                FROM `{PROJECT_ID}.{AI_DATASET}.prompt`
                WHERE prompt_type=@t AND project_name=@p
                ORDER BY prompt_name
            """,
                {"t": prompt_type, "p": project_name},
            )
        else:
            rows = bq_query(
                f"""
                SELECT DISTINCT prompt_name
                FROM `{PROJECT_ID}.{AI_DATASET}.prompt`
                WHERE prompt_type=@t
                ORDER BY prompt_name
            """,
                {"t": prompt_type},
            )
        return [r["prompt_name"] for r in rows]
    except Exception as e:
        st.warning(f"âš ï¸ {prompt_type} í”„ë¡¬í”„íŠ¸ ëª©ë¡ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        return []


def list_versions(
    prompt_type: str, prompt_name: str, project_name: str = None
) -> List[str]:
    """í”„ë¡¬í”„íŠ¸ ë²„ì „ ëª©ë¡ ì¡°íšŒ (í”„ë¡œì íŠ¸ë³„ í•„í„°ë§ ê°€ëŠ¥)"""
    if project_name:
        rows = bq_query(
            f"""
            SELECT version_name
            FROM `{PROJECT_ID}.{AI_DATASET}.prompt`
            WHERE prompt_type=@t AND prompt_name=@n AND project_name=@p
            ORDER BY created_at DESC
        """,
            {"t": prompt_type, "n": prompt_name, "p": project_name},
        )
    else:
        rows = bq_query(
            f"""
            SELECT version_name
            FROM `{PROJECT_ID}.{AI_DATASET}.prompt`
            WHERE prompt_type=@t AND prompt_name=@n
            ORDER BY created_at DESC
        """,
            {"t": prompt_type, "n": prompt_name},
        )
    return [r["version_name"] for r in rows]


def load_prompt_content(
    prompt_type: str, prompt_name: str, version_name: str, project_name: str = None
) -> str:
    """í”„ë¡¬í”„íŠ¸ ë‚´ìš© ë¡œë“œ (í”„ë¡œì íŠ¸ë³„ í•„í„°ë§ ê°€ëŠ¥)"""
    if project_name:
        rows = bq_query(
            f"""
            SELECT content
            FROM `{PROJECT_ID}.{AI_DATASET}.prompt`
            WHERE prompt_type=@t AND prompt_name=@n AND version_name=@v AND project_name=@p
            LIMIT 1
        """,
            {"t": prompt_type, "n": prompt_name, "v": version_name, "p": project_name},
        )
    else:
        rows = bq_query(
            f"""
            SELECT content
            FROM `{PROJECT_ID}.{AI_DATASET}.prompt`
            WHERE prompt_type=@t AND prompt_name=@n AND version_name=@v
            LIMIT 1
        """,
            {"t": prompt_type, "n": prompt_name, "v": version_name},
        )
    return rows[0]["content"] if rows else ""


def get_prompt_project(prompt_type: str, prompt_name: str, version_name: str) -> str:
    """í”„ë¡¬í”„íŠ¸ì˜ í”„ë¡œì íŠ¸ëª… ì¡°íšŒ"""
    rows = bq_query(
        f"""
        SELECT project_name
        FROM `{PROJECT_ID}.{AI_DATASET}.prompt`
        WHERE prompt_type=@t AND prompt_name=@n AND version_name=@v
        LIMIT 1
    """,
        {"t": prompt_type, "n": prompt_name, "v": version_name},
    )
    return rows[0]["project_name"] if rows and rows[0]["project_name"] else ""


def upsert_prompt_version(
    prompt_type: str,
    prompt_name: str,
    version_name: str,
    content: str,
    created_by: str,
    project_name: str = None,
):
    insert_rows_json(
        "prompt",
        [
            {
                "prompt_type": prompt_type,
                "prompt_name": prompt_name,
                "version_name": version_name,
                "content": content,
                "project_name": project_name,
                "is_active": True,
                "created_by": created_by,
                "content_md5": md5(content),
            }
        ],
    )


def update_project_name(old_project_name: str, new_project_name: str) -> bool:
    """í”„ë¡œì íŠ¸ëª… ì¼ê´„ ë³€ê²½"""
    try:
        # BigQuery í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        bq_client = bigquery.Client()

        # í”„ë¡œì íŠ¸ëª… ì—…ë°ì´íŠ¸ ì¿¼ë¦¬
        query = f"""
        UPDATE `{PROJECT_ID}.{AI_DATASET}.prompt` 
        SET project_name = @new_name
        WHERE project_name = @old_name
        """

        job_config = bigquery.QueryJobConfig(
            query_parameters=[
                bigquery.ScalarQueryParameter("new_name", "STRING", new_project_name),
                bigquery.ScalarQueryParameter("old_name", "STRING", old_project_name),
            ]
        )

        query_job = bq_client.query(query, job_config=job_config)
        query_job.result()  # ê²°ê³¼ ëŒ€ê¸°

        # ì—…ë°ì´íŠ¸ëœ í–‰ ìˆ˜ í™•ì¸
        rows_affected = query_job.num_dml_affected_rows
        st.success(
            f"âœ… í”„ë¡œì íŠ¸ëª… ë³€ê²½ ì™„ë£Œ! {rows_affected}ê°œ í”„ë¡¬í”„íŠ¸ê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤."
        )
        return True

    except Exception as e:
        st.error(f"âŒ í”„ë¡œì íŠ¸ëª… ë³€ê²½ ì‹¤íŒ¨: {e}")
        return False


# ---------- Cost estimator ----------
def estimate_cost_usd(model: str, in_tokens: int, out_tokens: int) -> float | None:
    # model_pricing í…Œì´ë¸”ì—ì„œ ê°€ì¥ ìµœì‹  ê°€ê²© ì •ë³´ ì¡°íšŒ (ai_sandbox ë°ì´í„°ì…‹ ì‚¬ìš©)
    rows = bq_query(
        f"""
        SELECT input_per_1k, output_per_1k
        FROM `{PROJECT_ID}.{AI_DATASET}.model_pricing`
        WHERE model_name=@m 
        AND (effective_to IS NULL OR effective_to > CURRENT_TIMESTAMP())
        ORDER BY effective_from DESC
        LIMIT 1
    """,
        {"m": model},
    )
    if not rows:
        return None
    pin, pout = rows[0]["input_per_1k"], rows[0]["output_per_1k"]
    if pin is None or pout is None:
        return None
    # Decimal íƒ€ì…ì„ floatìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ê³„ì‚°
    pin_float = float(pin)
    pout_float = float(pout)
    return (in_tokens / 1000) * pin_float + (out_tokens / 1000) * pout_float


# ---------- UI ----------
st.set_page_config(page_title="AI Prompt Test Bench", layout="wide")
st.title("ğŸ§ª AI Prompt Test Bench (GPT)")

with st.sidebar:
    st.subheader("ğŸ¤– ëª¨ë¸ ì„ íƒ")

    # OpenAI APIì—ì„œ ì‹¤ì‹œê°„ ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    st.caption("ğŸ”„ ì‹¤ì‹œê°„ ëª¨ë¸ ëª©ë¡ (10ë¶„ ìºì‹œ)")
    if st.button("ğŸ”„ ëª¨ë¸ ìƒˆë¡œê³ ì¹¨", key="refresh_models"):
        st.cache_data.clear()
        st.rerun()

    model_name = (
        st.selectbox("ëª¨ë¸ ì„ íƒ", get_available_models(), key="model_select")
        if get_available_models()
        else st.text_input("ëª¨ë¸ ì§ì ‘ ì…ë ¥", "gpt-4o-mini", key="model_input")
    )

    if get_available_models():
        st.caption(f"ì´ {len(get_available_models())}ê°œ ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥")

    st.markdown("---")
    st.subheader("ğŸ“ í”„ë¡¬í”„íŠ¸ ê´€ë¦¬")

    # í”„ë¡œì íŠ¸ ì„ íƒ ì„¹ì…˜
    st.caption("ğŸ“‚ í”„ë¡œì íŠ¸ ê´€ë¦¬")
    project_names = list_project_names()
    project_options = ["(ìƒˆ í”„ë¡œì íŠ¸)"] + project_names
    selected_project_option = st.selectbox(
        "í”„ë¡œì íŠ¸ ì„ íƒ", project_options, key="project_select"
    )

    if selected_project_option == "(ìƒˆ í”„ë¡œì íŠ¸)":
        current_project = st.text_input(
            "ìƒˆ í”„ë¡œì íŠ¸ëª…", "my_project", key="new_project_name"
        )
    else:
        current_project = selected_project_option
        # ê¸°ì¡´ í”„ë¡œì íŠ¸ ìˆ˜ì • ì˜µì…˜
        with st.expander("í”„ë¡œì íŠ¸ëª… ìˆ˜ì •"):
            new_project_name = st.text_input(
                "ìˆ˜ì •í•  í”„ë¡œì íŠ¸ëª…", current_project, key="edit_project_name"
            )
            if st.button("ğŸ“ í”„ë¡œì íŠ¸ëª… ë³€ê²½", key="update_project"):
                if new_project_name and new_project_name != current_project:
                    if update_project_name(current_project, new_project_name):
                        st.info(
                            f"í”„ë¡œì íŠ¸ëª…ì´ '{current_project}'ì—ì„œ '{new_project_name}'ìœ¼ë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤."
                        )
                        time.sleep(2)
                        st.rerun()
                else:
                    st.warning("ìƒˆë¡œìš´ í”„ë¡œì íŠ¸ëª…ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    # System Prompt ì„¹ì…˜
    st.caption("ğŸ­ System Prompt")
    sys_names = ["(ìƒˆë¡œ ì…ë ¥)"] + list_prompt_names(
        "system",
        current_project if selected_project_option != "(ìƒˆ í”„ë¡œì íŠ¸)" else None,
    )
    sys_name = st.selectbox("System í”„ë¡¬í”„íŠ¸ ì„ íƒ", sys_names, key="sys_name")
    sys_version = None
    sys_content = ""
    sys_project = current_project

    if sys_name != "(ìƒˆë¡œ ì…ë ¥)":
        versions = list_versions(
            "system",
            sys_name,
            current_project if selected_project_option != "(ìƒˆ í”„ë¡œì íŠ¸)" else None,
        )
        if versions:
            selected_sys_version = st.selectbox(
                "ë¶ˆëŸ¬ì˜¬ ë²„ì „", versions, key="sys_version_select"
            )
            sys_content = load_prompt_content(
                "system",
                sys_name,
                selected_sys_version,
                current_project if selected_project_option != "(ìƒˆ í”„ë¡œì íŠ¸)" else None,
            )
            # ê¸°ì¡´ í”„ë¡¬í”„íŠ¸ì˜ í”„ë¡œì íŠ¸ëª… ì¡°íšŒ
            existing_project = get_prompt_project(
                "system", sys_name, selected_sys_version
            )
            if existing_project:
                sys_project = existing_project

            # ì €ì¥í•  ë•Œ ì‚¬ìš©í•  ë²„ì „ëª… (ìˆ˜ì • ê°€ëŠ¥)
            sys_version = st.text_input(
                "ì €ì¥í•  ë²„ì „ëª…",
                selected_sys_version,
                key="new_sys_version",
                help="ê¸°ì¡´ ë²„ì „ì„ ìˆ˜ì •í•˜ê±°ë‚˜ ìƒˆ ë²„ì „ëª…ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: v1.0.1)",
            )
        else:
            st.warning("ì €ì¥ëœ ë²„ì „ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        sys_name = st.text_input("ìƒˆ System í”„ë¡¬í”„íŠ¸ëª…", "default", key="new_sys_name")
        sys_version = st.text_input("ìƒˆ ë²„ì „ëª…", "v1.0.0", key="new_sys_version")

    # User Prompt ì„¹ì…˜
    st.caption("ğŸ‘¤ User Prompt")
    usr_names = ["(ìƒˆë¡œ ì…ë ¥)"] + list_prompt_names(
        "user", current_project if selected_project_option != "(ìƒˆ í”„ë¡œì íŠ¸)" else None
    )
    usr_name = st.selectbox("User í”„ë¡¬í”„íŠ¸ ì„ íƒ", usr_names, key="usr_name")
    usr_version = None
    usr_content = ""
    usr_project = current_project

    if usr_name != "(ìƒˆë¡œ ì…ë ¥)":
        versions = list_versions(
            "user",
            usr_name,
            current_project if selected_project_option != "(ìƒˆ í”„ë¡œì íŠ¸)" else None,
        )
        if versions:
            selected_usr_version = st.selectbox(
                "ë¶ˆëŸ¬ì˜¬ ë²„ì „", versions, key="usr_version_select"
            )
            usr_content = load_prompt_content(
                "user",
                usr_name,
                selected_usr_version,
                current_project if selected_project_option != "(ìƒˆ í”„ë¡œì íŠ¸)" else None,
            )
            # ê¸°ì¡´ í”„ë¡¬í”„íŠ¸ì˜ í”„ë¡œì íŠ¸ëª… ì¡°íšŒ
            existing_project = get_prompt_project(
                "user", usr_name, selected_usr_version
            )
            if existing_project:
                usr_project = existing_project

            # ì €ì¥í•  ë•Œ ì‚¬ìš©í•  ë²„ì „ëª… (ìˆ˜ì • ê°€ëŠ¥)
            usr_version = st.text_input(
                "ì €ì¥í•  ë²„ì „ëª…",
                selected_usr_version,
                key="new_usr_version",
                help="ê¸°ì¡´ ë²„ì „ì„ ìˆ˜ì •í•˜ê±°ë‚˜ ìƒˆ ë²„ì „ëª…ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: v1.0.1)",
            )
        else:
            st.warning("ì €ì¥ëœ ë²„ì „ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        usr_name = st.text_input(
            "ìƒˆ User í”„ë¡¬í”„íŠ¸ëª…", "default_user", key="new_usr_name"
        )
        usr_version = st.text_input("ìƒˆ ë²„ì „ëª…", "v1.0.0", key="new_usr_version")

    st.markdown("---")
    st.subheader("ğŸ‘¨â€ğŸ’» í…ŒìŠ¤í„° ì •ë³´")
    tester = st.text_input("í…ŒìŠ¤í„°ëª…", os.environ.get("USER", ""), key="tester_name")

# ë©”ì¸ í¸ì§‘ ì˜ì—­
st.subheader("ğŸ“ í”„ë¡¬í”„íŠ¸ í¸ì§‘ ë° ì €ì¥")

# í˜„ì¬ í”„ë¡œì íŠ¸ ì •ë³´ í‘œì‹œ
if current_project:
    st.info(f"ğŸ“‚ **í˜„ì¬ í”„ë¡œì íŠ¸**: `{current_project}`")
else:
    st.warning("âš ï¸ í”„ë¡œì íŠ¸ë¥¼ ì„ íƒí•˜ê±°ë‚˜ ìƒˆë¡œ ì…ë ¥í•´ì£¼ì„¸ìš”.")

# 2ì—´ ë ˆì´ì•„ì›ƒìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ í¸ì§‘
col1, col2 = st.columns(2)

with col1:
    st.caption("ğŸ­ System Prompt")

    # í”„ë¡œì íŠ¸ëª… ìˆ˜ì • ì˜µì…˜ (ê¸°ì¡´ í”„ë¡¬í”„íŠ¸ì¸ ê²½ìš°)
    if sys_name != "(ìƒˆë¡œ ì…ë ¥)" and sys_name and sys_version:
        with st.expander("í”„ë¡œì íŠ¸ ì„¤ì •"):
            sys_project = st.text_input(
                "System í”„ë¡¬í”„íŠ¸ í”„ë¡œì íŠ¸ëª…",
                sys_project or current_project,
                key="sys_project_edit",
                help="ì´ í”„ë¡¬í”„íŠ¸ê°€ ì†í•  í”„ë¡œì íŠ¸ë¥¼ ì§€ì •í•˜ì„¸ìš”",
            )
    else:
        sys_project = current_project

    sys_content = st.text_area(
        "ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸",
        sys_content,
        height=200,
        placeholder="AIì˜ ì—­í• , ì„±ê²©, ê·œì¹™ ë“±ì„ ì •ì˜í•´ì£¼ì„¸ìš”...",
        key="sys_content_editor",
    )

    # System í”„ë¡¬í”„íŠ¸ ì €ì¥ ë²„íŠ¼
    if st.button(
        "ğŸ’¾ System í”„ë¡¬í”„íŠ¸ ì €ì¥",
        type="primary",
        use_container_width=True,
        key="save_sys",
    ):
        if sys_name and sys_version and sys_content and sys_project:
            upsert_prompt_version(
                "system",
                sys_name,
                sys_version,
                sys_content,
                tester or "unknown",
                sys_project,
            )
            st.success(f"âœ… System í”„ë¡¬í”„íŠ¸ ì €ì¥ ì™„ë£Œ! (í”„ë¡œì íŠ¸: {sys_project})")
            time.sleep(1)
            st.rerun()
        else:
            st.warning("System í”„ë¡¬í”„íŠ¸ ì •ë³´ì™€ í”„ë¡œì íŠ¸ëª…ì„ ëª¨ë‘ ì…ë ¥í•´ì£¼ì„¸ìš”.")

with col2:
    st.caption("ğŸ‘¤ User Prompt")

    # í”„ë¡œì íŠ¸ëª… ìˆ˜ì • ì˜µì…˜ (ê¸°ì¡´ í”„ë¡¬í”„íŠ¸ì¸ ê²½ìš°)
    if usr_name != "(ìƒˆë¡œ ì…ë ¥)" and usr_name and usr_version:
        with st.expander("í”„ë¡œì íŠ¸ ì„¤ì •"):
            usr_project = st.text_input(
                "User í”„ë¡¬í”„íŠ¸ í”„ë¡œì íŠ¸ëª…",
                usr_project or current_project,
                key="usr_project_edit",
                help="ì´ í”„ë¡¬í”„íŠ¸ê°€ ì†í•  í”„ë¡œì íŠ¸ë¥¼ ì§€ì •í•˜ì„¸ìš”",
            )
    else:
        usr_project = current_project

    usr_content = st.text_area(
        "ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸",
        usr_content,
        height=200,
        placeholder="ê¸°ë³¸ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”...",
        key="usr_content_editor",
    )

    # User í”„ë¡¬í”„íŠ¸ ì €ì¥ ë²„íŠ¼
    if st.button(
        "ğŸ’¾ User í”„ë¡¬í”„íŠ¸ ì €ì¥",
        type="primary",
        use_container_width=True,
        key="save_usr",
    ):
        if usr_name and usr_version and usr_content and usr_project:
            upsert_prompt_version(
                "user",
                usr_name,
                usr_version,
                usr_content,
                tester or "unknown",
                usr_project,
            )
            st.success(f"âœ… User í”„ë¡¬í”„íŠ¸ ì €ì¥ ì™„ë£Œ! (í”„ë¡œì íŠ¸: {usr_project})")
            time.sleep(1)
            st.rerun()
        else:
            st.warning("User í”„ë¡¬í”„íŠ¸ ì •ë³´ì™€ í”„ë¡œì íŠ¸ëª…ì„ ëª¨ë‘ ì…ë ¥í•´ì£¼ì„¸ìš”.")

# í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì„ íƒ
test_mode = st.radio(
    "ğŸ¯ **í…ŒìŠ¤íŠ¸ ë°©ì‹ì„ ì„ íƒí•˜ì„¸ìš”**",
    ["ğŸ’¬ ë‹¨ì¼ í…ŒìŠ¤íŠ¸ (ì¦‰ì‹œ ì‹¤í–‰)", "ğŸš€ ë°°ì¹˜ í…ŒìŠ¤íŠ¸ (ì—¬ëŸ¬ ë°ì´í„° ë™ì‹œ)"],
    horizontal=True,
    help="â€¢ ë‹¨ì¼ í…ŒìŠ¤íŠ¸: ìœ„ì—ì„œ ì…ë ¥í•œ ì‚¬ìš©ì ì§ˆë¬¸ìœ¼ë¡œ ì¦‰ì‹œ AI í…ŒìŠ¤íŠ¸\nâ€¢ ë°°ì¹˜ í…ŒìŠ¤íŠ¸: BigQueryì—ì„œ ì—¬ëŸ¬ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™€ì„œ í•œë²ˆì— í…ŒìŠ¤íŠ¸",
)

if test_mode == "ğŸ’¬ ë‹¨ì¼ í…ŒìŠ¤íŠ¸ (ì¦‰ì‹œ ì‹¤í–‰)":
    st.markdown("---")
    st.subheader("ğŸš€ ë‹¨ì¼ í…ŒìŠ¤íŠ¸ ì‹¤í–‰")

    # ì‚¬ìš©ì ì…ë ¥ ì˜ì—­
    user_input = st.text_area(
        "ì‚¬ìš©ì ì…ë ¥",
        "",
        height=120,
        placeholder="ì‹¤ì œ ì‚¬ìš©ìê°€ ì…ë ¥í•  ì§ˆë¬¸ì´ë‚˜ ìš”ì²­ì„ ì…ë ¥í•˜ì„¸ìš”...",
        key="user_input_text",
    )

    # ì‹¤í–‰ ë²„íŠ¼ë“¤
    col1, col2, col3 = st.columns([2, 1, 1])
    with col1:
        run_btn = st.button(
            "ğŸš€ AIì—ê²Œ ì§ˆë¬¸í•˜ê¸°", type="primary", use_container_width=True
        )
    with col2:
        clear_btn = st.button("ğŸ§¹ ì´ˆê¸°í™”", use_container_width=True)
    with col3:
        show_logs = st.button("ğŸ“Š ë¡œê·¸ ë³´ê¸°", use_container_width=True)

    if clear_btn:
        st.rerun()

    if run_btn:
        # ì…ë ¥ ìœ íš¨ì„± ê²€ì‚¬
        if not model_name:
            st.error("âŒ ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
            st.stop()

        if not sys_content.strip():
            st.error("âŒ System í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            st.stop()

        # ìµœì¢… ë©”ì‹œì§€ êµ¬ì„±
        final_user_content = usr_content + ("\n\n" + user_input if user_input else "")

        if not final_user_content.strip():
            st.error("âŒ ì‚¬ìš©ì ì…ë ¥ ë˜ëŠ” User í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            st.stop()

        messages = [
            {"role": "system", "content": sys_content},
            {"role": "user", "content": final_user_content},
        ]

        # ìš”ì²­ ë¯¸ë¦¬ë³´ê¸°
        with st.expander("ğŸ“‹ ìš”ì²­ ë¯¸ë¦¬ë³´ê¸°"):
            st.json({"model": model_name, "messages": messages})

        # OpenAI API í˜¸ì¶œ (ìŠ¤íŠ¸ë¦¬ë°)
        start = time.time()
        stream_area = st.empty()
        stream_text = ""

        try:
            # Chat Completions API (stream=True)
            response_stream = client.chat.completions.create(
                model=model_name, messages=messages, temperature=0.2, stream=True
            )

            response_data = None
            for chunk in response_stream:
                if chunk.choices[0].delta.content is not None:
                    stream_text += chunk.choices[0].delta.content
                    stream_area.markdown(f"**ğŸ¤– AI ì‘ë‹µ ì¤‘...**\n\n{stream_text}")

                # ë§ˆì§€ë§‰ ì²­í¬ì—ì„œ ì „ì²´ ì‘ë‹µ ì •ë³´ ìˆ˜ì§‘
                if chunk.choices[0].finish_reason is not None:
                    response_data = chunk

            latency_ms = int((time.time() - start) * 1000)

            # usage/ì‘ë‹µ ì •ë³´ ì²˜ë¦¬
            # ìŠ¤íŠ¸ë¦¬ë°ì—ì„œëŠ” usage ì •ë³´ê°€ ì œí•œì ì´ë¯€ë¡œ ëŒ€ëµì ìœ¼ë¡œ ê³„ì‚°
            usage = getattr(response_data, "usage", None) if response_data else None
            if usage:
                input_tokens = usage.prompt_tokens or 0
                output_tokens = usage.completion_tokens or 0
            else:
                # usage ì •ë³´ê°€ ì—†ìœ¼ë©´ ëŒ€ëµì ìœ¼ë¡œ ê³„ì‚° (1í† í° â‰ˆ 4ê¸€ì)
                input_tokens = len(" ".join([msg["content"] for msg in messages])) // 4
                output_tokens = len(stream_text) // 4

            # ë¹„ìš© ì¶”ì • (model_catalog ë‹¨ê°€ê°€ ìˆìœ¼ë©´)
            est_cost = estimate_cost_usd(model_name, input_tokens, output_tokens)

            # ìµœì¢… ì‘ë‹µ í‘œì‹œ
            stream_area.markdown(f"**âœ… ì™„ë£Œ!**\n\n{stream_text}")
            st.success(f"ì‘ë‹µ ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {latency_ms}ms)")

            # ì‘ë‹µ ìƒì„¸ ì •ë³´
            with st.expander("ğŸ“Š ì‘ë‹µ ìƒì„¸ ì •ë³´"):
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("ì§€ì—°ì‹œê°„", f"{latency_ms}ms")
                with col2:
                    st.metric("ì…ë ¥ í† í°", f"{input_tokens:,}")
                with col3:
                    st.metric("ì¶œë ¥ í† í°", f"{output_tokens:,}")

                if est_cost is not None:
                    st.metric("ì¶”ì • ë¹„ìš©", f"${est_cost:.6f}")

            # Raw Response ì •ë³´ (ê°„ì†Œí™”)
            with st.expander("ğŸ“¦ ìš”ì²­/ì‘ë‹µ JSON"):
                st.json(
                    {
                        "request": {"model": model_name, "messages": messages},
                        "response": {
                            "content": stream_text,
                            "usage": {
                                "prompt_tokens": input_tokens,
                                "completion_tokens": output_tokens,
                            },
                            "latency_ms": latency_ms,
                        },
                    }
                )

            # ë¡œê·¸ ì €ì¥
            insert_rows_json(
                "run_log",
                [
                    {
                        "tester": tester or "unknown",
                        "model_name": model_name,
                        "system_prompt_name": sys_name,
                        "system_prompt_version": sys_version,
                        "user_prompt_name": usr_name,
                        "user_prompt_version": usr_version,
                        "user_input": user_input,
                        "request_json": json.dumps(
                            {"messages": messages, "model": model_name},
                            ensure_ascii=False,
                        ),
                        "response_json": json.dumps(
                            {
                                "content": stream_text,
                                "usage": {
                                    "prompt_tokens": input_tokens,
                                    "completion_tokens": output_tokens,
                                },
                                "latency_ms": latency_ms,
                            },
                            ensure_ascii=False,
                        ),
                        "output_text": stream_text,
                        "input_tokens": int(input_tokens),
                        "output_tokens": int(output_tokens),
                        "latency_ms": int(latency_ms),
                        "est_cost_usd": est_cost,
                    }
                ],
            )

        except Exception as e:
            error_msg = str(e)
            stream_area.empty()

            # êµ¬ì²´ì ì¸ ì—ëŸ¬ ë©”ì‹œì§€ ì œê³µ
            if "api_key" in error_msg.lower():
                st.error("âŒ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                st.info("ğŸ’¡ í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEYë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
            elif "rate_limit" in error_msg.lower():
                st.error("âŒ API ìš”ì²­ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.")
                st.info("ğŸ’¡ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
            elif "model" in error_msg.lower() and "not found" in error_msg.lower():
                st.error(f"âŒ ëª¨ë¸ '{model_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                st.info("ğŸ’¡ ë‹¤ë¥¸ ëª¨ë¸ì„ ì„ íƒí•˜ê±°ë‚˜ ëª¨ë¸ëª…ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            elif "insufficient_quota" in error_msg.lower():
                st.error("âŒ OpenAI ê³„ì •ì˜ ì‚¬ìš©ëŸ‰ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.")
                st.info("ğŸ’¡ ê³„ì • ì‚¬ìš©ëŸ‰ì„ í™•ì¸í•˜ê±°ë‚˜ ê²°ì œ ì •ë³´ë¥¼ ì—…ë°ì´íŠ¸í•´ì£¼ì„¸ìš”.")
            else:
                st.error(f"âŒ OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {error_msg}")

            # ì—ëŸ¬ ë¡œê·¸ ì €ì¥
            try:
                insert_rows_json(
                    "run_log",
                    [
                        {
                            "tester": tester or "unknown",
                            "model_name": model_name,
                            "system_prompt_name": sys_name,
                            "system_prompt_version": sys_version,
                            "user_prompt_name": usr_name,
                            "user_prompt_version": usr_version,
                            "user_input": user_input,
                            "request_json": json.dumps(
                                {"messages": messages, "model": model_name},
                                ensure_ascii=False,
                            ),
                            "response_json": json.dumps(
                                {"error": error_msg}, ensure_ascii=False
                            ),
                            "output_text": f"ERROR: {error_msg}",
                            "input_tokens": 0,
                            "output_tokens": 0,
                            "latency_ms": int((time.time() - start) * 1000),
                            "est_cost_usd": 0.0,
                        }
                    ],
                )
            except Exception as log_error:
                st.warning(f"âš ï¸ ì—ëŸ¬ ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {log_error}")

    # ë¡œê·¸ ë³´ê¸° ê¸°ëŠ¥
    if "show_logs" in locals() and show_logs:
        st.markdown("---")
        st.subheader("ğŸ“Š ì‚¬ìš© ë¡œê·¸ ë° í†µê³„")

    try:
        # ìµœê·¼ 10ê°œ ë¡œê·¸ ì¡°íšŒ
        recent_logs = bq_query(
            f"""
            SELECT 
                ts,
                tester,
                model_name,
                system_prompt_name,
                user_prompt_name,
                SUBSTR(output_text, 1, 100) as output_preview,
                input_tokens,
                output_tokens,
                latency_ms,
                est_cost_usd
            FROM `{PROJECT_ID}.{AI_DATASET}.run_log`
            WHERE tester = @tester
            ORDER BY ts DESC
            LIMIT 10
        """,
            {"tester": tester or "unknown"},
        )

        if recent_logs:
            st.caption("ğŸ•’ ìµœê·¼ ì‹¤í–‰ ë¡œê·¸ (10ê°œ)")

            # ë¡œê·¸ í…Œì´ë¸” í‘œì‹œ
            import pandas as pd

            df = pd.DataFrame([dict(row) for row in recent_logs])
            df["ts"] = pd.to_datetime(df["ts"]).dt.strftime("%Y-%m-%d %H:%M:%S")
            df["output_preview"] = df["output_preview"].apply(
                lambda x: x[:50] + "..." if len(str(x)) > 50 else x
            )

            st.dataframe(
                df,
                column_config={
                    "ts": "ì‹¤í–‰ì‹œê°„",
                    "model_name": "ëª¨ë¸",
                    "system_prompt_name": "System í”„ë¡¬í”„íŠ¸",
                    "user_prompt_name": "User í”„ë¡¬í”„íŠ¸",
                    "output_preview": "ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°",
                    "input_tokens": st.column_config.NumberColumn(
                        "ì…ë ¥ í† í°", format="%d"
                    ),
                    "output_tokens": st.column_config.NumberColumn(
                        "ì¶œë ¥ í† í°", format="%d"
                    ),
                    "latency_ms": st.column_config.NumberColumn(
                        "ì§€ì—°ì‹œê°„(ms)", format="%d"
                    ),
                    "est_cost_usd": st.column_config.NumberColumn(
                        "ë¹„ìš©(USD)", format="$%.6f"
                    ),
                },
                hide_index=True,
                use_container_width=True,
            )

        # ì˜¤ëŠ˜ì˜ í†µê³„
        today_stats = bq_query(
            f"""
            SELECT 
                COUNT(*) as total_requests,
                SUM(input_tokens) as total_input_tokens,
                SUM(output_tokens) as total_output_tokens,
                SUM(est_cost_usd) as total_cost,
                AVG(latency_ms) as avg_latency,
                COUNT(DISTINCT model_name) as unique_models
            FROM `{PROJECT_ID}.{AI_DATASET}.run_log`
            WHERE DATE(ts) = CURRENT_DATE()
            AND tester = @tester
        """,
            {"tester": tester or "unknown"},
        )

        if today_stats and today_stats[0]["total_requests"] > 0:
            st.caption("ğŸ“ˆ ì˜¤ëŠ˜ì˜ í†µê³„")
            stats = today_stats[0]

            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("ì´ ìš”ì²­ ìˆ˜", f"{stats['total_requests']:,}")
            with col2:
                st.metric(
                    "ì´ í† í°",
                    f"{(stats['total_input_tokens'] or 0) + (stats['total_output_tokens'] or 0):,}",
                )
            with col3:
                st.metric("ì´ ë¹„ìš©", f"${stats['total_cost'] or 0:.4f}")
            with col4:
                st.metric("í‰ê·  ì§€ì—°ì‹œê°„", f"{stats['avg_latency'] or 0:.0f}ms")

        # ëª¨ë¸ë³„ ì‚¬ìš©ëŸ‰ í†µê³„
        model_stats = bq_query(
            f"""
            SELECT 
                model_name,
                COUNT(*) as requests,
                SUM(input_tokens) as input_tokens,
                SUM(output_tokens) as output_tokens,
                SUM(est_cost_usd) as cost,
                AVG(latency_ms) as avg_latency
            FROM `{PROJECT_ID}.{AI_DATASET}.run_log`
            WHERE DATE(ts) >= DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY)
            AND tester = @tester
            GROUP BY model_name
            ORDER BY requests DESC
        """,
            {"tester": tester or "unknown"},
        )

        if model_stats:
            st.caption("ğŸ¤– ìµœê·¼ 7ì¼ ëª¨ë¸ë³„ ì‚¬ìš©ëŸ‰")
            model_df = pd.DataFrame([dict(row) for row in model_stats])

            st.dataframe(
                model_df,
                column_config={
                    "model_name": "ëª¨ë¸ëª…",
                    "requests": st.column_config.NumberColumn("ìš”ì²­ìˆ˜", format="%d"),
                    "input_tokens": st.column_config.NumberColumn(
                        "ì…ë ¥ í† í°", format="%d"
                    ),
                    "output_tokens": st.column_config.NumberColumn(
                        "ì¶œë ¥ í† í°", format="%d"
                    ),
                    "cost": st.column_config.NumberColumn(
                        "ì´ ë¹„ìš©(USD)", format="$%.4f"
                    ),
                    "avg_latency": st.column_config.NumberColumn(
                        "í‰ê·  ì§€ì—°ì‹œê°„(ms)", format="%.0f"
                    ),
                },
                hide_index=True,
                use_container_width=True,
            )

        else:
            st.info("ğŸ“ ì•„ì§ ì‹¤í–‰ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤. AIì—ê²Œ ì§ˆë¬¸ì„ í•´ë³´ì„¸ìš”!")

    except Exception as e:
        st.error(f"âŒ ë¡œê·¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        st.info("ğŸ’¡ BigQuery ì—°ê²°ì„ í™•ì¸í•˜ê±°ë‚˜ í…Œì´ë¸”ì´ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")

# ---------- ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ê¸°ëŠ¥ ----------


def load_test_data(table_name: str, limit: int = 10) -> List[Dict[str, Any]]:
    """BigQueryì—ì„œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        rows = bq_query(
            f"""
            SELECT *
            FROM `{PROJECT_ID}.{BQ_DATASET}.{table_name}`
            LIMIT {limit}
        """
        )
        return [dict(row) for row in rows]
    except Exception as e:
        st.error(f"âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return []


def get_table_columns(table_name: str) -> List[str]:
    """í…Œì´ë¸”ì˜ ì»¬ëŸ¼ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        table_ref = bq.get_table(f"{PROJECT_ID}.{BQ_DATASET}.{table_name}")
        return [field.name for field in table_ref.schema]
    except Exception as e:
        st.error(f"âŒ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return []


def load_selected_test_data(
    table_name: str, selected_indices: List[int], input_column: str
) -> List[Dict[str, Any]]:
    """ì„ íƒëœ í–‰ì˜ ë°ì´í„°ë§Œ ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        # ëª¨ë“  ë°ì´í„°ë¥¼ ë¡œë“œí•œ í›„ ì„ íƒëœ ì¸ë±ìŠ¤ë§Œ í•„í„°ë§
        all_data = load_test_data(table_name, 1000)  # ì¶©ë¶„í•œ ë°ì´í„° ë¡œë“œ

        selected_data = []
        for idx in selected_indices:
            if idx < len(all_data):
                item = all_data[idx]
                # ì„ íƒëœ ì»¬ëŸ¼ì˜ ê°’ì„ user_inputìœ¼ë¡œ ì„¤ì •
                selected_data.append(
                    {
                        "id": f"row_{idx}",
                        "user_input": str(item.get(input_column, "")),
                        "original_data": item,  # ì›ë³¸ ë°ì´í„°ë„ ë³´ê´€
                    }
                )
        return selected_data
    except Exception as e:
        st.error(f"âŒ ì„ íƒëœ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return []


def list_test_tables() -> List[str]:
    """ì‚¬ìš© ê°€ëŠ¥í•œ í…ŒìŠ¤íŠ¸ ë°ì´í„° í…Œì´ë¸” ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        tables = bq.list_tables(BQ_DATASET)
        # 'test_' ë¡œ ì‹œì‘í•˜ëŠ” í…Œì´ë¸”ë“¤ë§Œ í•„í„°ë§
        test_tables = [table.table_id for table in tables if table.table_id]
        return sorted(test_tables)
    except Exception as e:
        st.warning(f"âš ï¸ í…ŒìŠ¤íŠ¸ í…Œì´ë¸” ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return []


def call_openai_api(
    model_name: str, messages: List[Dict], test_id: str, user_input: str = ""
) -> Dict[str, Any]:
    """ë‹¨ì¼ OpenAI API í˜¸ì¶œì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    start_time = time.time()
    try:
        response = client.chat.completions.create(
            model=model_name, messages=messages, temperature=0.2
        )

        latency_ms = int((time.time() - start_time) * 1000)

        # ì‘ë‹µ ì²˜ë¦¬
        content = response.choices[0].message.content
        usage = response.usage
        input_tokens = usage.prompt_tokens if usage else 0
        output_tokens = usage.completion_tokens if usage else 0

        # ë¹„ìš© ê³„ì‚°
        est_cost = estimate_cost_usd(model_name, input_tokens, output_tokens)
        # BigQuery NUMERIC ì •ë°€ë„ ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ ì†Œìˆ˜ì  6ìë¦¬ë¡œ ë°˜ì˜¬ë¦¼
        est_cost_rounded = round(est_cost, 6) if est_cost else 0.0

        return {
            "test_id": test_id,
            "status": "success",
            "content": content,
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "latency_ms": latency_ms,
            "est_cost_usd": est_cost_rounded,
            "error": None,
            "user_input": user_input,
            "request_json": {"messages": messages, "model": model_name},
        }

    except Exception as e:
        return {
            "test_id": test_id,
            "status": "error",
            "content": "",
            "input_tokens": 0,
            "output_tokens": 0,
            "latency_ms": int((time.time() - start_time) * 1000),
            "est_cost_usd": 0.0,
            "error": str(e),
            "user_input": user_input,
            "request_json": {"messages": messages, "model": model_name},
        }


def run_batch_test(
    model_name: str, sys_content: str, test_data: List[Dict], max_workers: int = 5
) -> List[Dict[str, Any]]:
    """ë°°ì¹˜ í…ŒìŠ¤íŠ¸ë¥¼ ë³‘ë ¬ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤."""

    # í”„ë¡œê·¸ë ˆìŠ¤ ë°” ì„¤ì •
    progress_bar = st.progress(0)
    status_text = st.empty()
    results = []

    def process_single_test(item):
        test_id = item.get("id", f"test_{len(results)}")
        user_input = item.get("user_input", item.get("input", str(item)))

        messages = [
            {"role": "system", "content": sys_content},
            {"role": "user", "content": user_input},
        ]

        return call_openai_api(model_name, messages, test_id, user_input)

    # ThreadPoolExecutorë¥¼ ì‚¬ìš©í•œ ë³‘ë ¬ ì²˜ë¦¬
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # ëª¨ë“  ì‘ì—… ì œì¶œ
        futures = [executor.submit(process_single_test, item) for item in test_data]

        # ê²°ê³¼ ìˆ˜ì§‘ ë° ì§„í–‰ìƒí™© ì—…ë°ì´íŠ¸
        for i, future in enumerate(futures):
            try:
                result = future.result(timeout=60)  # 60ì´ˆ íƒ€ì„ì•„ì›ƒ
                results.append(result)

                # ì§„í–‰ìƒí™© ì—…ë°ì´íŠ¸
                progress = (i + 1) / len(futures)
                progress_bar.progress(progress)
                status_text.text(f"ì§„í–‰ì¤‘... {i + 1}/{len(futures)} ì™„ë£Œ")

            except Exception as e:
                st.error(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
                results.append(
                    {"test_id": f"test_{i}", "status": "error", "error": str(e)}
                )

    progress_bar.progress(1.0)
    status_text.text("âœ… ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

    return results


def save_batch_results(
    results: List[Dict],
    model_name: str,
    sys_name: str,
    sys_version: str,
    usr_name: str,
    usr_version: str,
    tester: str,
):
    """ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ BigQueryì— ì €ì¥í•©ë‹ˆë‹¤."""

    batch_logs = []
    for result in results:
        if result["status"] == "success":
            batch_logs.append(
                {
                    "tester": tester or "unknown",
                    "model_name": model_name,
                    "system_prompt_name": sys_name,
                    "system_prompt_version": sys_version,
                    "user_prompt_name": usr_name,
                    "user_prompt_version": usr_version,
                    "user_input": result.get("user_input", ""),
                    "request_json": json.dumps(
                        result.get("request_json", {"model": model_name}),
                        ensure_ascii=False,
                    ),
                    "response_json": json.dumps(
                        {
                            "content": result["content"],
                            "usage": {
                                "prompt_tokens": result["input_tokens"],
                                "completion_tokens": result["output_tokens"],
                            },
                            "latency_ms": result["latency_ms"],
                        },
                        ensure_ascii=False,
                    ),
                    "output_text": result["content"],
                    "input_tokens": result["input_tokens"],
                    "output_tokens": result["output_tokens"],
                    "latency_ms": result["latency_ms"],
                    "est_cost_usd": (
                        round(result["est_cost_usd"], 6)
                        if result.get("est_cost_usd")
                        else 0.0
                    ),
                }
            )
        else:
            # ì—ëŸ¬ ë¡œê·¸
            batch_logs.append(
                {
                    "tester": tester or "unknown",
                    "model_name": model_name,
                    "system_prompt_name": sys_name,
                    "system_prompt_version": sys_version,
                    "user_prompt_name": usr_name,
                    "user_prompt_version": usr_version,
                    "user_input": result.get("user_input", ""),
                    "request_json": json.dumps(
                        result.get("request_json", {"model": model_name}),
                        ensure_ascii=False,
                    ),
                    "response_json": json.dumps(
                        {"error": result["error"]}, ensure_ascii=False
                    ),
                    "output_text": f"ERROR: {result['error']}",
                    "input_tokens": 0,
                    "output_tokens": 0,
                    "latency_ms": result.get("latency_ms", 0),
                    "est_cost_usd": 0.0,
                }
            )

    try:
        insert_rows_json("run_log", batch_logs)
        st.success(f"âœ… ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ê²°ê³¼ {len(batch_logs)}ê°œ ì €ì¥ ì™„ë£Œ!")
    except Exception as e:
        st.error(f"âŒ ë°°ì¹˜ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")


if test_mode == "ğŸš€ ë°°ì¹˜ í…ŒìŠ¤íŠ¸ (ì—¬ëŸ¬ ë°ì´í„° ë™ì‹œ)":
    st.subheader("ğŸš€ ë°°ì¹˜ í…ŒìŠ¤íŠ¸")
    st.caption("BigQueryì—ì„œ ì—¬ëŸ¬ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™€ì„œ ë™ì‹œì— ì‹¤í–‰í•©ë‹ˆë‹¤")

    batch_tab1, batch_tab2, batch_tab3 = st.tabs(
        ["ğŸ“‹ ë°ì´í„° ì„ íƒ", "âš™ï¸ ì‹¤í–‰ ì„¤ì •", "ğŸ“Š ê²°ê³¼"]
    )

    with batch_tab1:
        st.caption("ğŸ—ï¸ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„¤ì • ë° ì„ íƒ")

        # í•œ ì¤„ë¡œ ë°°ì¹˜ëœ ì„¤ì • ì˜µì…˜ë“¤
        col1, col2, col3 = st.columns([2, 2, 1])

        with col1:
            st.caption("ğŸ“Š í…Œì´ë¸” ì„ íƒ")
            test_tables = list_test_tables()
            if test_tables:
                selected_table = st.selectbox(
                    "í…ŒìŠ¤íŠ¸ ë°ì´í„° í…Œì´ë¸”",
                    test_tables,
                    key="batch_table",
                    label_visibility="collapsed",
                )
            else:
                st.warning("ğŸ“ 'gyg_'ë¡œ ì‹œì‘í•˜ëŠ” í…Œì´ë¸”ì´ ì—†ìŠµë‹ˆë‹¤.")
                selected_table = st.text_input(
                    "í…Œì´ë¸”ëª… ì§ì ‘ ì…ë ¥",
                    "gyg_test_data",
                    key="batch_table_manual",
                    label_visibility="collapsed",
                )

        with col2:
            st.caption("ğŸ”§ ì…ë ¥ ì»¬ëŸ¼ ì„ íƒ")
            if selected_table:
                columns = get_table_columns(selected_table)
                if columns:
                    input_column = st.selectbox(
                        "ì‚¬ìš©ì ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•  ì»¬ëŸ¼",
                        columns,
                        key="input_column",
                        help="ì´ ì»¬ëŸ¼ì˜ ê°’ì´ AIì—ê²Œ ì „ë‹¬ë  ì‚¬ìš©ì ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.",
                        label_visibility="collapsed",
                    )
                else:
                    st.error("âŒ ì»¬ëŸ¼ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    input_column = None
            else:
                input_column = None
                st.info("â† ë¨¼ì € í…Œì´ë¸”ì„ ì„ íƒí•˜ì„¸ìš”")

        with col3:
            st.caption("ğŸ“ ë¡œë“œ ê°œìˆ˜")
            preview_count = st.number_input(
                "ë¯¸ë¦¬ë³´ê¸° ë°ì´í„° ê°œìˆ˜",
                min_value=5,
                max_value=500,
                value=20,
                key="preview_count",
                help="ë¡œë“œí•  ë°ì´í„° ê°œìˆ˜ (ìµœëŒ€ 500ê°œ)",
                label_visibility="collapsed",
            )

        # ë°ì´í„° ë¡œë“œ ë²„íŠ¼ (ì „ì²´ ë„ˆë¹„)
        st.markdown("---")

        load_btn_disabled = not (selected_table and input_column)
        if st.button(
            "ğŸ“Š ë°ì´í„° ë¡œë“œ ë° ë¯¸ë¦¬ë³´ê¸°",
            key="load_preview_data",
            type="primary",
            use_container_width=True,
            disabled=load_btn_disabled,
        ):
            if selected_table and input_column:
                with st.spinner("ë°ì´í„° ë¡œë“œ ì¤‘..."):
                    preview_data = load_test_data(selected_table, preview_count)
                    st.session_state["preview_data"] = preview_data
                    st.session_state["current_table"] = selected_table
                    st.session_state["current_input_column"] = input_column

        if load_btn_disabled:
            st.caption("ğŸ’¡ í…Œì´ë¸”ê³¼ ì…ë ¥ ì»¬ëŸ¼ì„ ëª¨ë‘ ì„ íƒí•œ í›„ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì„¸ìš”.")

        # ë°ì´í„°ê°€ ë¡œë“œë˜ì—ˆìœ¼ë©´ ì„ íƒ ì¸í„°í˜ì´ìŠ¤ í‘œì‹œ
        if "preview_data" in st.session_state and st.session_state["preview_data"]:
            preview_data = st.session_state["preview_data"]
            # í˜„ì¬ ì„¤ì •ê³¼ ë¡œë“œëœ ë°ì´í„°ê°€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
            current_table = st.session_state.get("current_table")
            current_input_column = st.session_state.get("current_input_column")

            if current_table == selected_table and current_input_column == input_column:
                st.success(f"âœ… {len(preview_data)}ê°œ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
            else:
                st.warning("âš ï¸ ì„¤ì •ì´ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤. ë°ì´í„°ë¥¼ ë‹¤ì‹œ ë¡œë“œí•´ì£¼ì„¸ìš”.")
                st.session_state.pop("preview_data", None)
                st.stop()

                # ì „ì²´ ì„ íƒ/í•´ì œ ì²´í¬ë°•ìŠ¤
            col1, col2, col3 = st.columns([1, 1, 2])
            with col1:
                select_all = st.checkbox("ì „ì²´ ì„ íƒ", key="select_all")
            with col2:
                if st.button("ğŸ”„ ì„ íƒ ì´ˆê¸°í™”", key="clear_selection"):
                    st.session_state["selected_rows"] = []
                    st.rerun()

            # ì„ íƒëœ í–‰ ì¶”ì 
            if "selected_rows" not in st.session_state:
                st.session_state["selected_rows"] = []

            # ë°ì´í„° í…Œì´ë¸”ê³¼ ì²´í¬ë°•ìŠ¤
            st.markdown(
                "#### ğŸ“‹ ë°ì´í„° ì„ íƒ (ì²´í¬ë°•ìŠ¤ë¥¼ í´ë¦­í•˜ì—¬ í…ŒìŠ¤íŠ¸í•  ë°ì´í„° ì„ íƒ)"
            )

            # ë°ì´í„°ë¥¼ pandas DataFrameìœ¼ë¡œ ë³€í™˜
            import pandas as pd

            df = pd.DataFrame(preview_data)

            # ì²´í¬ë°•ìŠ¤ ì»¬ëŸ¼ ì¶”ê°€
            selected_indices = []

            # ì»¨í…Œì´ë„ˆë¡œ ìŠ¤í¬ë¡¤ ê°€ëŠ¥í•œ ì˜ì—­ ë§Œë“¤ê¸°
            with st.container(height=400):
                for idx, row in enumerate(preview_data):
                    col_check, col_data = st.columns([0.1, 0.9])

                    with col_check:
                        # ì „ì²´ ì„ íƒì´ ì²´í¬ë˜ì—ˆê±°ë‚˜ ê°œë³„ì ìœ¼ë¡œ ì„ íƒëœ ê²½ìš°
                        is_selected = select_all or idx in st.session_state.get(
                            "selected_rows", []
                        )

                        if st.checkbox(
                            "",
                            value=is_selected,
                            key=f"row_check_{idx}",
                            label_visibility="collapsed",
                        ):
                            if idx not in st.session_state["selected_rows"]:
                                st.session_state["selected_rows"].append(idx)
                            selected_indices.append(idx)
                        else:
                            if idx in st.session_state["selected_rows"]:
                                st.session_state["selected_rows"].remove(idx)

                    with col_data:
                        # í–‰ ë°ì´í„° í‘œì‹œ (ì…ë ¥ ì»¬ëŸ¼ ê°•ì¡°)
                        if input_column and input_column in row:
                            st.markdown(
                                f"**[í–‰ {idx}]** `{input_column}`: **{row[input_column]}**"
                            )
                            # ë‹¤ë¥¸ ì»¬ëŸ¼ë“¤ë„ ì‘ê²Œ í‘œì‹œ
                            other_cols = {
                                k: v for k, v in row.items() if k != input_column
                            }
                            if other_cols:
                                st.caption(f"ê¸°íƒ€: {other_cols}")
                        else:
                            st.markdown(f"**[í–‰ {idx}]** {row}")
                        st.markdown("---")

            # ì„ íƒ ìš”ì•½
            if select_all:
                final_selected = list(range(len(preview_data)))
            else:
                final_selected = st.session_state.get("selected_rows", [])

            st.info(f"ğŸ¯ ì„ íƒëœ ë°ì´í„°: {len(final_selected)}ê°œ")

            # ì„ íƒëœ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ë‹¤ìŒ íƒ­ìœ¼ë¡œ ì´ë™ ì•ˆë‚´
            if final_selected:
                st.success("âœ… ë°ì´í„° ì„ íƒ ì™„ë£Œ! 'âš™ï¸ ì‹¤í–‰ ì„¤ì •' íƒ­ìœ¼ë¡œ ì´ë™í•˜ì„¸ìš”.")
                # ì„ íƒëœ ë°ì´í„°ë¥¼ ì„¸ì…˜ì— ì €ì¥
                st.session_state["final_selected_indices"] = final_selected
                st.session_state["selected_table"] = selected_table
                st.session_state["selected_input_column"] = input_column

    with batch_tab2:
        st.caption("âš™ï¸ ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì„¤ì •")

        # ì„ íƒëœ ë°ì´í„° í™•ì¸
        if (
            "final_selected_indices" not in st.session_state
            or "selected_table" not in st.session_state
            or "selected_input_column" not in st.session_state
        ):
            st.warning("âš ï¸ ë¨¼ì € 'ğŸ“‹ ë°ì´í„° ì„ íƒ' íƒ­ì—ì„œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
        else:
            selected_count = len(st.session_state["final_selected_indices"])
            table_name = st.session_state["selected_table"]
            input_col = st.session_state["selected_input_column"]

            st.success(
                f"âœ… ì„ íƒëœ ë°ì´í„°: {selected_count}ê°œ (í…Œì´ë¸”: {table_name}, ì»¬ëŸ¼: {input_col})"
            )

            # ì‹¤í–‰ ì„¤ì •
            col1, col2 = st.columns(2)

            with col1:
                st.markdown("#### ğŸ”§ ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •")
                max_workers = st.number_input(
                    "ë™ì‹œ ì‹¤í–‰ ìˆ˜",
                    min_value=1,
                    max_value=min(20, selected_count),
                    value=min(5, selected_count),
                    key="batch_workers",
                )
                st.caption(f"âš ï¸ ìµœëŒ€ {min(20, selected_count)}ê°œê¹Œì§€ ê°€ëŠ¥")

            with col2:
                st.markdown("#### ğŸ“Š ì˜ˆìƒ ë¹„ìš©")
                if model_name:
                    # ëŒ€ëµì ì¸ í† í° ìˆ˜ ì¶”ì • (1 ë¬¸ì â‰ˆ 0.75 í† í°ìœ¼ë¡œ ê°€ì •)
                    avg_input_length = 100  # ê¸°ë³¸ê°’
                    if "preview_data" in st.session_state and input_col:
                        lengths = []
                        for idx in st.session_state["final_selected_indices"]:
                            if idx < len(st.session_state["preview_data"]):
                                text = str(
                                    st.session_state["preview_data"][idx].get(
                                        input_col, ""
                                    )
                                )
                                lengths.append(len(text))
                        if lengths:
                            avg_input_length = sum(lengths) / len(lengths)

                    estimated_input_tokens = (
                        int(avg_input_length * 0.75) + len(sys_content.split())
                        if sys_content
                        else 0
                    )
                    estimated_output_tokens = 150  # ì¶”ì •ê°’

                    single_cost = estimate_cost_usd(
                        model_name, estimated_input_tokens, estimated_output_tokens
                    )
                    if single_cost:
                        total_estimated_cost = single_cost * selected_count
                        st.metric("ì˜ˆìƒ ì´ ë¹„ìš©", f"${total_estimated_cost:.4f}")
                        st.caption(f"í…ŒìŠ¤íŠ¸ë‹¹ ì•½ ${single_cost:.6f}")

            st.markdown("---")

            # ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë²„íŠ¼
            if st.button(
                f"ğŸš€ AIì—ê²Œ ì§ˆë¬¸í•˜ê¸° ({selected_count}ê°œ)",
                type="primary",
                use_container_width=True,
                key="run_batch",
            ):
                if not model_name:
                    st.error("âŒ ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
                elif not sys_content.strip():
                    st.error("âŒ System í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                else:
                    # ì„ íƒëœ ë°ì´í„° ë¡œë“œ
                    st.info(f"ğŸ“‹ ì„ íƒëœ {selected_count}ê°œ ë°ì´í„° ì¤€ë¹„ ì¤‘...")

                    selected_test_data = load_selected_test_data(
                        table_name,
                        st.session_state["final_selected_indices"],
                        input_col,
                    )

                    if not selected_test_data:
                        st.error("âŒ ì„ íƒëœ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    else:
                        st.success(
                            f"âœ… {len(selected_test_data)}ê°œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ"
                        )

                        # ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
                        st.info(f"âš¡ ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì‹œì‘... (ë™ì‹œ ì‹¤í–‰: {max_workers}ê°œ)")

                        batch_results = run_batch_test(
                            model_name=model_name,
                            sys_content=sys_content,
                            test_data=selected_test_data,
                            max_workers=max_workers,
                        )

                        # ê²°ê³¼ ì €ì¥
                        save_batch_results(
                            results=batch_results,
                            model_name=model_name,
                            sys_name=sys_name,
                            sys_version=sys_version,
                            usr_name=usr_name,
                            usr_version=usr_version,
                            tester=tester,
                        )

                        # ì„¸ì…˜ ìƒíƒœì— ê²°ê³¼ ì €ì¥ (ê²°ê³¼ íƒ­ì—ì„œ í‘œì‹œ)
                        st.session_state["batch_results"] = batch_results

                        st.success(
                            "ğŸ‰ ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì™„ë£Œ! 'ğŸ“Š ê²°ê³¼' íƒ­ì—ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”."
                        )

    with batch_tab3:
        results = st.session_state.get("batch_results", [])

        if results:
            # ê²°ê³¼ ìš”ì•½
            success_count = sum(1 for r in results if r["status"] == "success")
            error_count = len(results) - success_count
            total_cost = sum(r.get("est_cost_usd", 0) for r in results)
            avg_latency = (
                sum(r.get("latency_ms", 0) for r in results) / len(results)
                if results
                else 0
            )

            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("ì„±ê³µ", success_count)
            with col2:
                st.metric("ì‹¤íŒ¨", error_count)
            with col3:
                st.metric("ì´ ë¹„ìš©", f"${total_cost:.4f}")
            with col4:
                st.metric("í‰ê·  ì§€ì—°ì‹œê°„", f"{avg_latency:.0f}ms")

            # ìƒì„¸ ê²°ê³¼ í…Œì´ë¸”
            st.caption("ğŸ“Š ìƒì„¸ ê²°ê³¼")
            import pandas as pd

            df_results = pd.DataFrame(results)
            st.dataframe(
                df_results,
                column_config={
                    "test_id": "í…ŒìŠ¤íŠ¸ ID",
                    "status": "ìƒíƒœ",
                    "content": st.column_config.TextColumn("ì‘ë‹µ", width="large"),
                    "input_tokens": st.column_config.NumberColumn("ì…ë ¥ í† í°"),
                    "output_tokens": st.column_config.NumberColumn("ì¶œë ¥ í† í°"),
                    "latency_ms": st.column_config.NumberColumn("ì§€ì—°ì‹œê°„(ms)"),
                    "est_cost_usd": st.column_config.NumberColumn(
                        "ë¹„ìš©(USD)", format="$%.6f"
                    ),
                    "error": "ì—ëŸ¬",
                },
                hide_index=True,
                use_container_width=True,
            )

            # ê²°ê³¼ ë‹¤ìš´ë¡œë“œ
            csv = df_results.to_csv(index=False)
            st.download_button(
                label="ğŸ“¥ ê²°ê³¼ CSV ë‹¤ìš´ë¡œë“œ",
                data=csv,
                file_name=f"batch_test_results_{int(time.time())}.csv",
                mime="text/csv",
            )
        else:
            st.info("ğŸ“ ë°°ì¹˜ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•˜ë©´ ê²°ê³¼ê°€ ì—¬ê¸°ì— í‘œì‹œë©ë‹ˆë‹¤.")
